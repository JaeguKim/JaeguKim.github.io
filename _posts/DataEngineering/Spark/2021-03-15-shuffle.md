---
layout: post
title: "Shuffle"
date: 2021-03-15 00:48:00 +0900
categories: [DataEngineering,Spark]
---

## Spark Architecture: Shuffle

![Spark Shuffle Design](https://i2.wp.com/0x0fff.com/wp-content/uploads/2015/08/Spark-Shuffle-Design.png?resize=317%2C289)

Shuffle 이란 무엇일까? 하루에 발생한 전화량을 계산한다고 가정해보자. 이 경우 "day"를 key로 설정할것이고, 각각의 value는 1로 설정할것이다. 그리고 각각의 key에 대해서 value들을 합할것이다. 하지만 데이터를 클러스터에 저장할때, 같은 key에 대한 value들을 어떻게 계산할 수 있을까? 유일한 방법은 같은 key에 대한 value들이 같은 머신에 있도록 하는 방법 뿐이다.

이 주제를 논의하기에 앞서, 이 글에서는 MapReduce naming covention을 따르도록 하겠다. shuffle에서 source executor에서 데이터를 내보내는 task를 "mapper" 라 부르고 target executor로 데이터를 consume하는 task를 "reducer" 라 하고 둘 사이에 발생하는 것을 "shuffle" 이라 부르겠다.

Shuffling은 일반적으로 2가지 중요한 compression parameter가 존재한다 : ```spark.shuffle.compress``` - engine이 shuffle output을 압축할지 말지, ```spark.shuffle.spill.compress``` - intermediate shuffle spill file을 압축할지 말지. 두가지의 기본 설정은 true이다 그리고 ```spark.io.compression.codec``` 은 snappy로 설정되어있다.

Spark에는 많은 shuffle 구현방법이 존재한다. 어떤 구현을 사용할지는 ```spark.shuffle.manager``` 파라미터로 결정된다. 3가지 옵션이 존재한다 : hash,sort,tungsten-sort 그리고 "sort" 옵션이 기본값(>= spark 1.2.0)이다.

## Hash Shuffle

Spark 1.2.0 이전에는 shuffle의 기본 옵션이었다. 많은 단점이 있는데, 파일의 수가 많아 지는것이 원인이다. 각각의 mapper task가 각각의 reducer에 대해서 독립적인 파일을 생성하고, 결과적으로 M*R 개의 파일을 생성한다. 이때 M은 mapper의 수이고 R은 reducer의 수이다. mapper와 reducer 수가 커지면 output buffer size, open file 수, 파일들을 생성하고 삭제하는 속도 등의 관점에서 이는 큰 문제가 된다. 

![spark_hash_shuffle_no_consolidation](https://i1.wp.com/0x0fff.com/wp-content/uploads/2015/08/spark_hash_shuffle_no_consolidation-1024x484.png?resize=625%2C295)

이를 최적화하는 방법이 하나 존재한다, ```spark.shuffle.consolidateFiles``` 파라미터로 결정이 되는데 기본값은 false이다. true로 설정이 되면 "mapper" output 파일들이 하나로 모이게 된다. 만약 클러스터가 E개의 executor가 존재하고 그리고 C개의 core가 존재하고 각각의 task가 T개의 CPU를 요청한다면, execution slot의 수는 ```E*C/T``` 가 될것이다. 그리고 shuffle 하는 동안 생성되는 파일 수는 ```E*C/T*R``` 이다. 만약 100개의 executor가 존재하고 10개의 core가 있고 task당 1개의 core를 할당하고 46000 "reducer" 가 있다고 가정하면, 2 billion 파일수에서 46million 파일 수로 줄일 수 있다. reducer 각각에 대해서 새로운 파일을 생성하는 대신에, output file의 pool 을 생성한다. map task가 데이터를 출력하기 시작하면, 이 pool에서 R개의 파일 그룹을 요청한다. 이 작업이 끝나면, R개의 파일 그룹을 다시 pool에 돌려준다. 각각의 executor가 C/T task를 병렬로 실행하기 때문에, 오직 C/T개의  output file 그룹만 생성하게 된다. 각각의 group은 R개의 파일로 구성된다. 처음 C/T 개의 "map" task가 끝나면 다음 "map" task는 pool 존재하는 그룹을 재사용하게 된다.

![spark_hash_shuffle_with_consolidation](https://i1.wp.com/0x0fff.com/wp-content/uploads/2015/08/spark_hash_shuffle_with_consolidation-1024x500.png?resize=625%2C305)

### 장점

1. 빠르다 - sorting이 필요없고 hash table이 유지된다.
2. 데이터를 소팅하는 메모리 overhead가 없다.
3. IO overhead가 없다 - 데이터는 HDD에 정확히 한번 기록되고 정확히 한번 읽어진다.

### 단점

1. 파티션양이 커질때, 퍼포먼스는 떨어진다.(많은 output file이 생성 되므로)
2. 다량의 파일은 random IO에 대한 IO skew를 야기한다. 일반적으로 sequential IO보다 100배 느리다.